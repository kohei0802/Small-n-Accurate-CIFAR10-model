{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-06T14:59:06.503594Z","iopub.execute_input":"2025-07-06T14:59:06.503941Z","iopub.status.idle":"2025-07-06T14:59:06.509441Z","shell.execute_reply.started":"2025-07-06T14:59:06.503917Z","shell.execute_reply":"2025-07-06T14:59:06.508539Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"import torch \nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T14:59:09.182172Z","iopub.execute_input":"2025-07-06T14:59:09.182893Z","iopub.status.idle":"2025-07-06T14:59:09.186528Z","shell.execute_reply.started":"2025-07-06T14:59:09.182867Z","shell.execute_reply":"2025-07-06T14:59:09.185869Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 64\n\n# Training transforms with augmentation\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # 50% chance to flip\n    transforms.RandomCrop(32, padding=4),  # Pad then randomly crop\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random color adjustments\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 mean/std\n])\n\n# Test transforms (no augmentation)\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\ntraining_data = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=True,\n    transform=transform_train,  # Use augmentation for training\n    download=True\n)\n\ntesting_data = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=False,\n    transform=transform_test  # No augmentation for testing\n)\n\ntraining_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\ntesting_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)\n\nx, y = next(iter(training_loader))\nprint(x.shape)\nprint(y.shape)\nprint(y)\n\nprint(len(testing_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:29:16.439010Z","iopub.execute_input":"2025-07-06T16:29:16.439308Z","iopub.status.idle":"2025-07-06T16:29:17.760650Z","shell.execute_reply.started":"2025-07-06T16:29:16.439289Z","shell.execute_reply":"2025-07-06T16:29:17.760033Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 32, 32])\ntorch.Size([64])\ntensor([1, 7, 6, 4, 8, 5, 6, 5, 3, 5, 4, 1, 7, 5, 7, 9, 6, 0, 7, 2, 9, 9, 7, 9,\n        5, 6, 4, 2, 1, 8, 0, 1, 8, 8, 2, 5, 8, 4, 6, 8, 0, 4, 4, 5, 5, 2, 5, 3,\n        4, 9, 9, 3, 1, 6, 3, 1, 9, 8, 4, 3, 3, 9, 0, 0])\n157\n","output_type":"stream"}],"execution_count":127},{"cell_type":"code","source":"class Classifier(nn.Module):\n  def __init__(self, H, W, output_size):\n    super().__init__()\n    self.layer = nn.Sequential(\n        nn.Conv2d(3, 48, 3, padding=1),\n        nn.Conv2d(48, 48, 3, padding=1),\n        nn.BatchNorm2d(48),\n        nn.LeakyReLU(),\n        nn.MaxPool2d(2),\n        nn.Conv2d(48, 64, 3, padding=1),\n        nn.BatchNorm2d(64),\n        nn.LeakyReLU(),\n        nn.MaxPool2d(2),\n        nn.Dropout2d(0.1),\n        nn.Conv2d(64, 128, 3, padding=1),\n        nn.BatchNorm2d(128),\n        nn.LeakyReLU(),\n        nn.MaxPool2d(2),\n        nn.Dropout(0.2),\n        nn.Flatten(),\n        nn.Dropout(0.3),\n        nn.Linear(128 * H//8 * W//8, 100),\n        nn.LeakyReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(100, output_size),\n    )\n\n  def forward(self, x):\n    out = self.layer(x)\n    return out\n\nmodel = Classifier(32, 32, 10).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T17:08:58.320042Z","iopub.execute_input":"2025-07-06T17:08:58.320535Z","iopub.status.idle":"2025-07-06T17:08:58.332763Z","shell.execute_reply.started":"2025-07-06T17:08:58.320513Z","shell.execute_reply":"2025-07-06T17:08:58.332211Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"n_epochs = 50\nlearning_rate = 0.001\npatience = 5  # Number of epochs to wait before early stopping\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n\nbest_val_loss = float('inf')\npatience_counter = 0\nbest_model_state = None\n\nfor epoch in range(n_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    for i, (x, y) in enumerate(training_loader):\n        x, y = x.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for x, y in testing_loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            val_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(training_loader)\n    avg_val_loss = val_loss / len(testing_loader)\n    \n    print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n    \n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        best_model_state = model.state_dict().copy()\n    else:\n        patience_counter += 1\n        \n    if patience_counter >= patience:\n        print('Early stopping!')\n        break\n\n# Load best model\nmodel.load_state_dict(best_model_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T17:09:03.085488Z","iopub.execute_input":"2025-07-06T17:09:03.086148Z","iopub.status.idle":"2025-07-06T17:40:23.579236Z","shell.execute_reply.started":"2025-07-06T17:09:03.086122Z","shell.execute_reply":"2025-07-06T17:40:23.578310Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss: 1.7500, Val Loss: 1.3373\nEpoch 2: Train Loss: 1.4223, Val Loss: 1.0653\nEpoch 3: Train Loss: 1.2661, Val Loss: 0.9675\nEpoch 4: Train Loss: 1.1661, Val Loss: 0.8629\nEpoch 5: Train Loss: 1.0949, Val Loss: 0.8529\nEpoch 6: Train Loss: 1.0285, Val Loss: 0.7720\nEpoch 7: Train Loss: 0.9831, Val Loss: 0.7348\nEpoch 8: Train Loss: 0.9465, Val Loss: 0.7102\nEpoch 9: Train Loss: 0.9108, Val Loss: 0.6734\nEpoch 10: Train Loss: 0.8828, Val Loss: 0.6565\nEpoch 11: Train Loss: 0.8521, Val Loss: 0.6385\nEpoch 12: Train Loss: 0.8367, Val Loss: 0.6564\nEpoch 13: Train Loss: 0.8142, Val Loss: 0.6181\nEpoch 14: Train Loss: 0.8074, Val Loss: 0.5902\nEpoch 15: Train Loss: 0.7867, Val Loss: 0.5892\nEpoch 16: Train Loss: 0.7747, Val Loss: 0.5724\nEpoch 17: Train Loss: 0.7625, Val Loss: 0.5646\nEpoch 18: Train Loss: 0.7505, Val Loss: 0.5824\nEpoch 19: Train Loss: 0.7380, Val Loss: 0.5495\nEpoch 20: Train Loss: 0.7302, Val Loss: 0.5426\nEpoch 21: Train Loss: 0.7234, Val Loss: 0.5267\nEpoch 22: Train Loss: 0.7195, Val Loss: 0.5390\nEpoch 23: Train Loss: 0.7163, Val Loss: 0.5215\nEpoch 24: Train Loss: 0.7101, Val Loss: 0.5183\nEpoch 25: Train Loss: 0.6979, Val Loss: 0.5114\nEpoch 26: Train Loss: 0.7013, Val Loss: 0.5002\nEpoch 27: Train Loss: 0.6896, Val Loss: 0.5014\nEpoch 28: Train Loss: 0.6906, Val Loss: 0.4961\nEpoch 29: Train Loss: 0.6798, Val Loss: 0.5094\nEpoch 30: Train Loss: 0.6769, Val Loss: 0.4852\nEpoch 31: Train Loss: 0.6731, Val Loss: 0.4863\nEpoch 32: Train Loss: 0.6720, Val Loss: 0.4958\nEpoch 33: Train Loss: 0.6658, Val Loss: 0.5002\nEpoch 34: Train Loss: 0.6641, Val Loss: 0.4826\nEpoch 35: Train Loss: 0.6589, Val Loss: 0.5061\nEpoch 36: Train Loss: 0.6583, Val Loss: 0.4857\nEpoch 37: Train Loss: 0.6584, Val Loss: 0.4824\nEpoch 38: Train Loss: 0.6500, Val Loss: 0.4710\nEpoch 39: Train Loss: 0.6507, Val Loss: 0.4623\nEpoch 40: Train Loss: 0.6422, Val Loss: 0.4761\nEpoch 41: Train Loss: 0.6486, Val Loss: 0.4588\nEpoch 42: Train Loss: 0.6396, Val Loss: 0.4617\nEpoch 43: Train Loss: 0.6458, Val Loss: 0.4758\nEpoch 44: Train Loss: 0.6327, Val Loss: 0.4665\nEpoch 45: Train Loss: 0.6356, Val Loss: 0.4627\nEpoch 46: Train Loss: 0.6306, Val Loss: 0.4602\nEarly stopping!\n","output_type":"stream"},{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":"def evaluate_accuracy(model, data_loader, device):\n    model.eval()  # Set to evaluation mode\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # No need to track gradients\n        for x, y in data_loader:\n            x = x.to(device)\n            y = y.to(device)\n            \n            outputs = model(x)\n            _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n            \n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n    \n    return 100 * correct / total  # Return accuracy percentage\n\n# Check accuracy on training set\ntrain_accuracy = evaluate_accuracy(model, training_loader, device)\nprint(f'Training Accuracy: {train_accuracy:.2f}%')\n\n# Check accuracy on test set\ntest_accuracy = evaluate_accuracy(model, testing_loader, device)\nprint(f'Test Accuracy: {test_accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T17:57:42.147914Z","iopub.execute_input":"2025-07-06T17:57:42.148569Z","iopub.status.idle":"2025-07-06T17:58:17.092805Z","shell.execute_reply.started":"2025-07-06T17:57:42.148533Z","shell.execute_reply":"2025-07-06T17:58:17.092024Z"}},"outputs":[{"name":"stdout","text":"Training Accuracy: 84.99%\nTest Accuracy: 84.51%\n","output_type":"stream"}],"execution_count":137}]}